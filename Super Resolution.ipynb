{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDSR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A seguinte célula é a implementação do modelo EDSR (Enhanced Deep residual networks fo single image Super Resolution). Este modelo se caracteriza pelo uso de blocos residuais antes do upsample. Ele obteve melhores resultados que seus antecessores ao remover camdas de Batch Normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.layers import Add, Conv2D, Input, Lambda\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "DIV2K_RGB_MEAN = np.array([0.4488, 0.4371, 0.4040]) * 255\n",
    "\n",
    "# Directory for saving model weights\n",
    "weights_dir = 'weights'\n",
    "\n",
    "def edsr(scale, num_filters=64, num_res_blocks=8, res_block_scaling=None):\n",
    "    \"\"\"Creates an EDSR model.\"\"\"\n",
    "    x_in = Input(shape=(None, None, 3))\n",
    "    x = Lambda(normalize)(x_in)\n",
    "\n",
    "    x = b = Conv2D(num_filters, 3, padding='same')(x)\n",
    "    for i in range(num_res_blocks):\n",
    "        b = res_block(b, num_filters, res_block_scaling)\n",
    "    b = Conv2D(num_filters, 3, padding='same')(b)\n",
    "    x = Add()([x, b])\n",
    "\n",
    "    x = upsample(x, scale, num_filters)\n",
    "    x = Conv2D(3, 3, padding='same')(x)\n",
    "\n",
    "    x = Lambda(denormalize)(x)\n",
    "    return Model(x_in, x, name=\"edsr\")\n",
    "\n",
    "\n",
    "def res_block(x_in, filters, scaling):\n",
    "    \"\"\"Creates an EDSR residual block.\"\"\"\n",
    "    x = Conv2D(filters, 3, padding='same', activation='relu')(x_in)\n",
    "    x = Conv2D(filters, 3, padding='same')(x)\n",
    "    if scaling:\n",
    "        x = Lambda(lambda t: t * scaling)(x)\n",
    "    x = Add()([x_in, x])\n",
    "    return x\n",
    "\n",
    "\n",
    "def upsample(x, scale, num_filters):\n",
    "    def upsample_1(x, factor, **kwargs):\n",
    "        \"\"\"Sub-pixel convolution.\"\"\"\n",
    "        x = Conv2D(num_filters * (factor ** 2), 3, padding='same', **kwargs)(x)\n",
    "        return Lambda(pixel_shuffle(scale=factor))(x)\n",
    "\n",
    "    if scale == 2:\n",
    "        x = upsample_1(x, 2, name='conv2d_1_scale_2')\n",
    "    elif scale == 3:\n",
    "        x = upsample_1(x, 3, name='conv2d_1_scale_3')\n",
    "    elif scale == 4:\n",
    "        x = upsample_1(x, 2, name='conv2d_1_scale_2')\n",
    "        x = upsample_1(x, 2, name='conv2d_2_scale_2')\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def pixel_shuffle(scale):\n",
    "    return lambda x: tf.nn.depth_to_space(x, scale)\n",
    "\n",
    "\n",
    "def normalize(x):\n",
    "    return (x - DIV2K_RGB_MEAN) / 127.5\n",
    "\n",
    "\n",
    "def denormalize(x):\n",
    "    return x * 127.5 + DIV2K_RGB_MEAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download dp dataset de treino DIV2K. Primeiramente todas as imagens recebem downsize em um fator 4. Depois disso elas recebem transformações aleatórias como rotações e inversões para aumentar variabilidade do dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import DIV2K\n",
    "\n",
    "train = DIV2K(scale=4, downgrade='bicubic', subset='train')\n",
    "train_ds = train.dataset(batch_size=16, random_transform=True)\n",
    "\n",
    "valid = DIV2K(scale=4, downgrade='bicubic', subset='valid')\n",
    "valid_ds = valid.dataset(batch_size=1, random_transform=False, repeat_count=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primeiro treinamento da rede, usando uma loss function constante. A função usada será a diferença média por pixel ou L1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers.schedules import PiecewiseConstantDecay\n",
    "\n",
    "# EDSR baseline as described in the EDSR paper (1.52M parameters)\n",
    "model_edsr = edsr(scale=4, num_res_blocks=32, num_filters=256)\n",
    "\n",
    "# Adam optimizer with a scheduler that halfs learning rate after 200,000 steps\n",
    "optim_edsr = Adam(learning_rate=PiecewiseConstantDecay(boundaries=[200000], values=[1e-4, 5e-5]))\n",
    "\n",
    "# Compile and train model for 300,000 steps with L1 pixel loss\n",
    "model_edsr.compile(optimizer=optim_edsr, loss='mean_absolute_error')\n",
    "model_edsr.fit(train_ds, epochs=300, steps_per_epoch=1000)\n",
    "\n",
    "# Save model weights\n",
    "model_edsr.save_weights(os.path.join(weights_dir, 'weights-edsr-32-x4.h5'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualização do resultado da rede no seu estado atual. Pode ser visto que as imagens geradas são melhores que algoritmos tradicionais mas ainda não são nítidas. Isso ocorre porque a loss function é constante e favorece imagens que não erram muito em cada pixel mas acabam também não acertando completamente em nenhum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from model import resolve_single\n",
    "from utils import load_image\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "edsr_pre_trained = edsr(scale=4, num_res_blocks=16)\n",
    "edsr_pre_trained.load_weights(os.path.join(weights_dir, 'weights-edsr-16-x4.h5'))\n",
    "lr = load_image('demo/0869x4-crop.png')\n",
    "sr = resolve_single(edsr_pre_trained, lr)\n",
    "\n",
    "plt.figure(figsize=(20, 20))\n",
    "    \n",
    "model_name = edsr_pre_trained.name.upper()\n",
    "images = [lr, sr]\n",
    "titles = ['LR', f'SR ({model_name}, pixel loss)']\n",
    "positions = [1, 3]\n",
    "    \n",
    "for i, (image, title, position) in enumerate(zip(images, titles, positions)):\n",
    "    plt.subplot(2, 2, position)\n",
    "    plt.imshow(image)\n",
    "    plt.title(title)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para resolver esses problemas será feito uma segunda leva de treinos. O modelo pré-treinado será o gerador e usaremos uma outra rede para ser o discriminador fazendo um modelo adversarial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import srgan\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers.schedules import PiecewiseConstantDecay\n",
    "import os\n",
    "\n",
    "# Used in content_loss\n",
    "mean_squared_error = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "# Used in generator_loss and discriminator_loss\n",
    "binary_cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "\n",
    "# Model that computes the feature map after the 4th convolution \n",
    "# before the 5th max-pooling layer in VGG19. This is layer 20 in\n",
    "# the corresponding Keras model.\n",
    "vgg = srgan.vgg_54()\n",
    "\n",
    "# EDSR model used as generator in SRGAN\n",
    "generator = edsr(scale=4, num_res_blocks=32, num_filters=256)\n",
    "#generator.load_weights(os.path.join(weights_dir, 'weights-edsr-32-x4.h5'))\n",
    "generator.load_weights(os.path.join(weights_dir, 'weights-edsr-32-x4-fine-tuned.h5'))\n",
    "\n",
    "# SRGAN discriminator\n",
    "discriminator = srgan.discriminator()\n",
    "\n",
    "# Optmizers for generator and discriminator. SRGAN will be trained for\n",
    "# 200,000 steps and learning rate is reduced from 1e-4 to 1e-5 after\n",
    "# 100,000 steps\n",
    "schedule = PiecewiseConstantDecay(boundaries=[100000], values=[1e-4, 1e-5])\n",
    "generator_optimizer = Adam(learning_rate=schedule)\n",
    "discriminator_optimizer = Adam(learning_rate=schedule)\n",
    "\n",
    "def generator_loss(sr_out):\n",
    "    return binary_cross_entropy(tf.ones_like(sr_out), sr_out)\n",
    "\n",
    "def discriminator_loss(hr_out, sr_out):\n",
    "    hr_loss = binary_cross_entropy(tf.ones_like(hr_out), hr_out)\n",
    "    sr_loss = binary_cross_entropy(tf.zeros_like(sr_out), sr_out)\n",
    "    return hr_loss + sr_loss\n",
    "\n",
    "@tf.function\n",
    "def content_loss(hr, sr):\n",
    "    sr = tf.keras.applications.vgg19.preprocess_input(sr)\n",
    "    hr = tf.keras.applications.vgg19.preprocess_input(hr)\n",
    "    sr_features = vgg(sr) / 12.75\n",
    "    hr_features = vgg(hr) / 12.75\n",
    "    return mean_squared_error(hr_features, sr_features)\n",
    "\n",
    "@tf.function\n",
    "def train_step(lr, hr):\n",
    "    \"\"\"SRGAN training step.\n",
    "    \n",
    "    Takes an LR and an HR image batch as input and returns\n",
    "    the computed perceptual loss and discriminator loss.\n",
    "    \"\"\"\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        lr = tf.cast(lr, tf.float32)\n",
    "        hr = tf.cast(hr, tf.float32)\n",
    "\n",
    "        # Forward pass\n",
    "        sr = generator(lr, training=True)\n",
    "        hr_output = discriminator(hr, training=True)\n",
    "        sr_output = discriminator(sr, training=True)\n",
    "\n",
    "        # Compute losses\n",
    "        con_loss = content_loss(hr, sr)\n",
    "        gen_loss = generator_loss(sr_output)\n",
    "        perc_loss = con_loss + 0.001 * gen_loss\n",
    "        disc_loss = discriminator_loss(hr_output, sr_output)\n",
    "\n",
    "    # Compute gradient of perceptual loss w.r.t. generator weights \n",
    "    gradients_of_generator = gen_tape.gradient(perc_loss, generator.trainable_variables)\n",
    "    # Compute gradient of discriminator loss w.r.t. discriminator weights \n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "    # Update weights of generator and discriminator\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "\n",
    "    return perc_loss, disc_loss\n",
    "\n",
    "pls_metric = tf.keras.metrics.Mean()\n",
    "dls_metric = tf.keras.metrics.Mean()\n",
    "\n",
    "steps = 200000\n",
    "#step = 0\n",
    "step = 17000\n",
    "\n",
    "# Train SRGAN for 200,000 steps.\n",
    "for lr, hr in train_ds.take(steps):\n",
    "    step += 1\n",
    "\n",
    "    pl, dl = train_step(lr, hr)\n",
    "    pls_metric(pl)\n",
    "    dls_metric(dl)\n",
    "\n",
    "    if step % 50 == 0:\n",
    "        print(f'{step}/{steps}, perceptual loss = {pls_metric.result():.4f}, discriminator loss = {dls_metric.result():.4f}')\n",
    "        pls_metric.reset_states()\n",
    "        dls_metric.reset_states()\n",
    "    if step % 500 == 0:\n",
    "        generator.save_weights(os.path.join(weights_dir, 'weights-edsr-32-x4-fine-tuned.h5'))\n",
    "        \n",
    "generator.save_weights(os.path.join(weights_dir, 'weights-edsr-32-x4-fine-tuned.h5'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E agora podemos comparar os resultados e ver o quão significativo foi o salto de qualidade do modelo refinado em comparação com o pré-treinado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from model import resolve_single\n",
    "from utils import load_image\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "def resolve_and_plot(model_pre_trained, model_fine_tuned, lr_image_path):\n",
    "    lr = load_image(lr_image_path)\n",
    "    \n",
    "    sr_pt = resolve_single(model_pre_trained, lr)\n",
    "    sr_ft = resolve_single(model_fine_tuned, lr)\n",
    "    \n",
    "    plt.figure(figsize=(20, 20))\n",
    "    \n",
    "    model_name = model_pre_trained.name.upper()\n",
    "    images = [lr, sr_pt, sr_ft]\n",
    "    titles = ['LR', f'SR ({model_name}, pixel loss)', f'SR ({model_name}, perceptual loss)']\n",
    "    positions = [1, 3, 4]\n",
    "    \n",
    "    for i, (image, title, position) in enumerate(zip(images, titles, positions)):\n",
    "        plt.subplot(2, 2, position)\n",
    "        plt.imshow(image)\n",
    "        plt.title(title)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edsr_pre_trained = edsr(scale=4, num_res_blocks=16)\n",
    "edsr_pre_trained.load_weights(os.path.join(weights_dir, 'weights-edsr-16-x4.h5'))\n",
    "\n",
    "edsr_fine_tuned = edsr(scale=4, num_res_blocks=16)\n",
    "edsr_fine_tuned.load_weights(os.path.join(weights_dir, 'weights-edsr-16-x4-fine-tuned.h5'))\n",
    "\n",
    "resolve_and_plot(edsr_pre_trained, edsr_fine_tuned, 'demo/0869x4-crop.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E, por último, uma célula para visualização de imagens maiores. Dependendo do tamanho da imagem é necessário dividí-la e ampliar cada parte separadamente por limitações de VRAM da GPU. Isto, obviamente, varia dependendo da máquina."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from model import resolve_single\n",
    "from utils import load_image\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "edsr_fine_tuned = edsr(scale=4, num_res_blocks=16)\n",
    "edsr_fine_tuned.load_weights(os.path.join(weights_dir, 'weights-edsr-16-x4-fine-tuned.h5'))\n",
    "\n",
    "lr = load_image(\"images/LowRes/ExampleLargeImage.jpeg\")\n",
    "lr = Image.fromarray(lr)\n",
    "\n",
    "(lef, up, right, down) = lr.getbbox()\n",
    "\n",
    "steps = 8\n",
    "\n",
    "newWidth = right * 4\n",
    "newHeight = down *4\n",
    "oldStepX = right/steps\n",
    "oldStepY = down/steps\n",
    "newStepX = newWidth/steps\n",
    "newStepY = newHeight/steps\n",
    "\n",
    "\n",
    "sr = Image.new(\"RGB\", (newWidth, newHeight))\n",
    "\n",
    "for i in range(0, steps):\n",
    "    for j in range(0, steps):\n",
    "        pre = lr.crop((j*oldStepX, i*oldStepY, (j+1)*oldStepX, (i+1)*oldStepY))\n",
    "        pre = np.array(pre)\n",
    "        post = resolve_single(edsr_fine_tuned, pre)\n",
    "        post = np.array(post)\n",
    "        post = Image.fromarray(post)\n",
    "        sr.paste(post, (int(j*newStepX), int(i*newStepY)))\n",
    "\n",
    "#sr.save(\"Images/HighRes/ExampleLargeImage.jpg\")\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "lr4x = load_image(\"images/LowRes/ExampleLargeImage4x.jpg\")\n",
    "\n",
    "plt.figure(figsize=(20, 20))\n",
    "    \n",
    "model_name = edsr_fine_tuned.name.upper()\n",
    "images = [lr, lr4x, sr]\n",
    "titles = ['LR', 'Bicubic Interpolation', 'EDSR']\n",
    "positions = [1, 3, 4]\n",
    "    \n",
    "for i, (image, title, position) in enumerate(zip(images, titles, positions)):\n",
    "    plt.subplot(2, 2, position)\n",
    "    plt.imshow(image)\n",
    "    plt.title(title)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu4",
   "language": "python",
   "name": "gpu4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
