{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WDSR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from data import DIV2K\n",
    "from model.wdsr import wdsr_b\n",
    "from train import WdsrTrainer\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of residual blocks\n",
    "depth = 32\n",
    "\n",
    "# Super-resolution factor\n",
    "scale = 4\n",
    "\n",
    "# Downgrade operator\n",
    "downgrade = 'bicubic'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Location of model weights (needed for demo)\n",
    "weights_dir = \"weights/wdsr-b-32-x4\"\n",
    "weights_file = os.path.join(weights_dir, 'weights.h5')\n",
    "\n",
    "os.makedirs(weights_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets\n",
    "\n",
    "You don't need to download the DIV2K dataset as the required parts are automatically downloaded by the `DIV2K` class. By default, DIV2K images are stored in folder `.div2k` in the project's root directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "div2k_train = DIV2K(scale=scale, subset='train', downgrade=downgrade)\n",
    "div2k_valid = DIV2K(scale=scale, subset='valid', downgrade=downgrade)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = div2k_train.dataset(batch_size=16, random_transform=True)\n",
    "valid_ds = div2k_valid.dataset(batch_size=1, random_transform=False, repeat_count=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "### Pre-trained models\n",
    "\n",
    "If you want to skip training and directly run the demo below, download [weights-wdsr-b-32-x4.tar.gz](https://drive.google.com/open?id=1JfQNGQZ9cG-lyC5EB0W3MpySjPlDJXpU) and extract the archive in the project's root directory. This will create a `weights/wdsr-b-32-x4` directory containing the weights of the pre-trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = WdsrTrainer(model=wdsr_b(scale=scale, num_res_blocks=depth), \n",
    "                      checkpoint_dir=f'.ckpt/wdsr-b-{depth}-x{scale}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train WDSR B model for 300,000 steps and evaluate model\n",
    "# every 1000 steps on the first 10 images of the DIV2K\n",
    "# validation set. Save a checkpoint only if evaluation\n",
    "# PSNR has improved.\n",
    "trainer.train(train_ds,\n",
    "              valid_ds.take(10),\n",
    "              steps=300000, \n",
    "              evaluate_every=1000, \n",
    "              save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restore from checkpoint with highest PSNR\n",
    "trainer.restore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model on full validation set\n",
    "psnr = trainer.evaluate(valid_ds)\n",
    "print(f'PSNR = {psnr.numpy():3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save weights to separate location (needed for demo)\n",
    "trainer.model.save_weights(weights_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import srgan\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers.schedules import PiecewiseConstantDecay\n",
    "import os\n",
    "\n",
    "# Used in content_loss\n",
    "mean_squared_error = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "# Used in generator_loss and discriminator_loss\n",
    "binary_cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "\n",
    "# Model that computes the feature map after the 4th convolution \n",
    "# before the 5th max-pooling layer in VGG19. This is layer 20 in\n",
    "# the corresponding Keras model.\n",
    "vgg = srgan.vgg_54()\n",
    "\n",
    "# EDSR model used as generator in SRGAN\n",
    "generator = wdsr_b(scale=scale, num_res_blocks=depth)\n",
    "generator.load_weights(weights_file)\n",
    "\n",
    "# SRGAN discriminator\n",
    "discriminator = srgan.discriminator()\n",
    "\n",
    "# Optmizers for generator and discriminator. SRGAN will be trained for\n",
    "# 200,000 steps and learning rate is reduced from 1e-4 to 1e-5 after\n",
    "# 100,000 steps\n",
    "schedule = PiecewiseConstantDecay(boundaries=[100000], values=[1e-4, 1e-5])\n",
    "generator_optimizer = Adam(learning_rate=schedule)\n",
    "discriminator_optimizer = Adam(learning_rate=schedule)\n",
    "\n",
    "def generator_loss(sr_out):\n",
    "    return binary_cross_entropy(tf.ones_like(sr_out), sr_out)\n",
    "\n",
    "def discriminator_loss(hr_out, sr_out):\n",
    "    hr_loss = binary_cross_entropy(tf.ones_like(hr_out), hr_out)\n",
    "    sr_loss = binary_cross_entropy(tf.zeros_like(sr_out), sr_out)\n",
    "    return hr_loss + sr_loss\n",
    "\n",
    "@tf.function\n",
    "def content_loss(hr, sr):\n",
    "    sr = tf.keras.applications.vgg19.preprocess_input(sr)\n",
    "    hr = tf.keras.applications.vgg19.preprocess_input(hr)\n",
    "    sr_features = vgg(sr) / 12.75\n",
    "    hr_features = vgg(hr) / 12.75\n",
    "    return mean_squared_error(hr_features, sr_features)\n",
    "\n",
    "@tf.function\n",
    "def train_step(lr, hr):\n",
    "    \"\"\"SRGAN training step.\n",
    "    \n",
    "    Takes an LR and an HR image batch as input and returns\n",
    "    the computed perceptual loss and discriminator loss.\n",
    "    \"\"\"\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        lr = tf.cast(lr, tf.float32)\n",
    "        hr = tf.cast(hr, tf.float32)\n",
    "\n",
    "        # Forward pass\n",
    "        sr = generator(lr, training=True)\n",
    "        hr_output = discriminator(hr, training=True)\n",
    "        sr_output = discriminator(sr, training=True)\n",
    "\n",
    "        # Compute losses\n",
    "        con_loss = content_loss(hr, sr)\n",
    "        gen_loss = generator_loss(sr_output)\n",
    "        perc_loss = con_loss + 0.001 * gen_loss\n",
    "        disc_loss = discriminator_loss(hr_output, sr_output)\n",
    "\n",
    "    # Compute gradient of perceptual loss w.r.t. generator weights \n",
    "    gradients_of_generator = gen_tape.gradient(perc_loss, generator.trainable_variables)\n",
    "    # Compute gradient of discriminator loss w.r.t. discriminator weights \n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "    # Update weights of generator and discriminator\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "\n",
    "    return perc_loss, disc_loss\n",
    "\n",
    "pls_metric = tf.keras.metrics.Mean()\n",
    "dls_metric = tf.keras.metrics.Mean()\n",
    "\n",
    "steps = 200000\n",
    "step = 0\n",
    "\n",
    "# Train SRGAN for 200,000 steps.\n",
    "for lr, hr in train_ds.take(steps):\n",
    "    step += 1\n",
    "\n",
    "    print(step)\n",
    "    pl, dl = train_step(lr, hr)\n",
    "    print(\"a\")\n",
    "    pls_metric(pl)\n",
    "    dls_metric(dl)\n",
    "\n",
    "    if step % 50 == 0:\n",
    "        print(f'{step}/{steps}, perceptual loss = {pls_metric.result():.4f}, discriminator loss = {dls_metric.result():.4f}')\n",
    "        pls_metric.reset_states()\n",
    "        dls_metric.reset_states()\n",
    "        \n",
    "generator.save_weights(os.path.join(weights_dir, 'weights-fine-tuned.h5'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = wdsr_b(scale=scale, num_res_blocks=depth)\n",
    "model.load_weights(weights_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import resolve_single\n",
    "from utils import load_image, plot_sample\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "def resolve_and_plot(lr_image_path, save=False, name=\"\"):\n",
    "    lr = load_image(lr_image_path)\n",
    "    sr = resolve_single(model, lr)\n",
    "    plot_sample(lr, sr)\n",
    "    if save:\n",
    "        sr = np.array(sr)\n",
    "        sr = Image.fromarray(sr)\n",
    "        sr.save(\"Images/HighRes/\" + name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resolve_and_plot('demo/0869x4-crop.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resolve_and_plot('demo/0829x4-crop.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "resolve_and_plot('demo/0851x4-crop.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
